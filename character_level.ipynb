{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LMg6rJ0rQ_W",
        "outputId": "1e19b2ed-638c-4192-9bf0-cf7393d42766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "recipes = pd.read_csv(\"drive/MyDrive/Fall 2024: Study Abroad/Recipes Generator/Data/recipes_data.csv\")"
      ],
      "metadata": {
        "id": "uzqBvYqwrZy8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library imports\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras.layers import LSTM, Dense"
      ],
      "metadata": {
        "id": "ZbJmvd3xKkWg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "\n",
        "recipes = recipes[:10000]\n",
        "recipes = recipes[recipes[\"NER\"].str.len() <= 100]\n",
        "\n",
        "ingredients_raw = []\n",
        "names_raw = []\n",
        "ingrs_chars = set()\n",
        "names_chars = set()\n",
        "\n",
        "for (ingredients, name) in zip(recipes[\"NER\"], recipes[\"title\"]):\n",
        "  ingredients_raw.append(''.join(ingredients))\n",
        "  name = '\\t' + name + '\\n'\n",
        "  names_raw.append(name)\n",
        "\n",
        "  for char in ingredients:\n",
        "    if char not in ingrs_chars:\n",
        "        ingrs_chars.add(char)\n",
        "  for char in name:\n",
        "    if char not in names_chars:\n",
        "        names_chars.add(char)\n",
        "\n",
        "ingrs_chars = sorted(list(ingrs_chars))\n",
        "names_chars = sorted(list(names_chars))\n",
        "num_encoder_tokens = len(ingrs_chars)\n",
        "num_decoder_tokens = len(names_chars)\n",
        "max_encoder_seq_length = max([len(txt) for txt in ingredients_raw])\n",
        "max_decoder_seq_length = max([len(txt) for txt in names_raw])\n",
        "\n",
        "print(\"Number of recipes:\", len(ingredients_raw))\n",
        "print(\"Number of unique ingredient tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique name tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for ingredients:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for names:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MIamlXkrdhi",
        "outputId": "18fa38fb-20f3-4361-bd7c-6ba1f4a6b959"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of recipes: 6307\n",
            "Number of unique ingredient tokens: 39\n",
            "Number of unique name tokens: 78\n",
            "Max sequence length for ingredients: 100\n",
            "Max sequence length for names: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "\n",
        "ingrs_token_index = dict([(char, i) for i, char in enumerate(ingrs_chars)])\n",
        "names_token_index = dict([(char, i) for i, char in enumerate(names_chars)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(ingredients_raw), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(ingredients_raw), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(ingredients_raw), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (ingredients, name) in enumerate(zip(ingredients_raw, names_raw)):\n",
        "    for t, char in enumerate(ingredients):\n",
        "        encoder_input_data[i, t, ingrs_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, ingrs_token_index[\" \"]] = 1.0\n",
        "\n",
        "    for t, char in enumerate(name):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, names_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, names_token_index[char]] = 1.0\n",
        "\n",
        "    decoder_input_data[i, t + 1 :, names_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, names_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "jt3x_LE-KMUD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 256\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "8YXHSs9JTplv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TIiNviDXKXR",
        "outputId": "ae52dd91-dcbc-4824-fce2-70db621f1082"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.7519 - loss: 1.4321 - val_accuracy: 0.8068 - val_loss: 0.8865\n",
            "Epoch 2/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8045 - loss: 0.8133 - val_accuracy: 0.8056 - val_loss: 0.7643\n",
            "Epoch 3/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8070 - loss: 0.7651 - val_accuracy: 0.8096 - val_loss: 0.7062\n",
            "Epoch 4/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8094 - loss: 0.7089 - val_accuracy: 0.8102 - val_loss: 0.6766\n",
            "Epoch 5/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8123 - loss: 0.6810 - val_accuracy: 0.8149 - val_loss: 0.6471\n",
            "Epoch 6/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8207 - loss: 0.6329 - val_accuracy: 0.8307 - val_loss: 0.5929\n",
            "Epoch 7/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8302 - loss: 0.5938 - val_accuracy: 0.8309 - val_loss: 0.5738\n",
            "Epoch 8/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8377 - loss: 0.5609 - val_accuracy: 0.8458 - val_loss: 0.5322\n",
            "Epoch 9/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8455 - loss: 0.5339 - val_accuracy: 0.8503 - val_loss: 0.5132\n",
            "Epoch 10/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8508 - loss: 0.5132 - val_accuracy: 0.8559 - val_loss: 0.4933\n",
            "Epoch 11/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8557 - loss: 0.4956 - val_accuracy: 0.8586 - val_loss: 0.4844\n",
            "Epoch 12/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8615 - loss: 0.4761 - val_accuracy: 0.8620 - val_loss: 0.4709\n",
            "Epoch 13/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8644 - loss: 0.4664 - val_accuracy: 0.8692 - val_loss: 0.4529\n",
            "Epoch 14/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8679 - loss: 0.4556 - val_accuracy: 0.8673 - val_loss: 0.4520\n",
            "Epoch 15/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8701 - loss: 0.4496 - val_accuracy: 0.8716 - val_loss: 0.4382\n",
            "Epoch 16/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8744 - loss: 0.4331 - val_accuracy: 0.8737 - val_loss: 0.4336\n",
            "Epoch 17/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8758 - loss: 0.4286 - val_accuracy: 0.8781 - val_loss: 0.4169\n",
            "Epoch 18/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8800 - loss: 0.4133 - val_accuracy: 0.8807 - val_loss: 0.4092\n",
            "Epoch 19/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8819 - loss: 0.4090 - val_accuracy: 0.8839 - val_loss: 0.4010\n",
            "Epoch 20/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8831 - loss: 0.4056 - val_accuracy: 0.8859 - val_loss: 0.3946\n",
            "Epoch 21/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8861 - loss: 0.3948 - val_accuracy: 0.8869 - val_loss: 0.3913\n",
            "Epoch 22/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8877 - loss: 0.3896 - val_accuracy: 0.8885 - val_loss: 0.3845\n",
            "Epoch 23/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8901 - loss: 0.3799 - val_accuracy: 0.8902 - val_loss: 0.3790\n",
            "Epoch 24/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8919 - loss: 0.3733 - val_accuracy: 0.8919 - val_loss: 0.3727\n",
            "Epoch 25/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8934 - loss: 0.3697 - val_accuracy: 0.8924 - val_loss: 0.3703\n",
            "Epoch 26/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8939 - loss: 0.3661 - val_accuracy: 0.8933 - val_loss: 0.3686\n",
            "Epoch 27/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8961 - loss: 0.3558 - val_accuracy: 0.8950 - val_loss: 0.3607\n",
            "Epoch 28/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8978 - loss: 0.3523 - val_accuracy: 0.8971 - val_loss: 0.3561\n",
            "Epoch 29/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9004 - loss: 0.3461 - val_accuracy: 0.8988 - val_loss: 0.3509\n",
            "Epoch 30/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9001 - loss: 0.3472 - val_accuracy: 0.8998 - val_loss: 0.3473\n",
            "Epoch 31/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9020 - loss: 0.3390 - val_accuracy: 0.9005 - val_loss: 0.3439\n",
            "Epoch 32/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9047 - loss: 0.3318 - val_accuracy: 0.9022 - val_loss: 0.3383\n",
            "Epoch 33/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9069 - loss: 0.3213 - val_accuracy: 0.9030 - val_loss: 0.3366\n",
            "Epoch 34/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9068 - loss: 0.3215 - val_accuracy: 0.9029 - val_loss: 0.3348\n",
            "Epoch 35/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9070 - loss: 0.3210 - val_accuracy: 0.9054 - val_loss: 0.3288\n",
            "Epoch 36/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9071 - loss: 0.3207 - val_accuracy: 0.9057 - val_loss: 0.3273\n",
            "Epoch 37/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9100 - loss: 0.3115 - val_accuracy: 0.9047 - val_loss: 0.3266\n",
            "Epoch 38/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9109 - loss: 0.3069 - val_accuracy: 0.9072 - val_loss: 0.3236\n",
            "Epoch 39/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9120 - loss: 0.3052 - val_accuracy: 0.9076 - val_loss: 0.3224\n",
            "Epoch 40/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9122 - loss: 0.3029 - val_accuracy: 0.9074 - val_loss: 0.3179\n",
            "Epoch 41/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9127 - loss: 0.3017 - val_accuracy: 0.9093 - val_loss: 0.3138\n",
            "Epoch 42/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9132 - loss: 0.3003 - val_accuracy: 0.9096 - val_loss: 0.3138\n",
            "Epoch 43/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9141 - loss: 0.2963 - val_accuracy: 0.9103 - val_loss: 0.3094\n",
            "Epoch 44/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9160 - loss: 0.2897 - val_accuracy: 0.9102 - val_loss: 0.3109\n",
            "Epoch 45/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9166 - loss: 0.2878 - val_accuracy: 0.9107 - val_loss: 0.3081\n",
            "Epoch 46/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9169 - loss: 0.2853 - val_accuracy: 0.9102 - val_loss: 0.3089\n",
            "Epoch 47/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9174 - loss: 0.2837 - val_accuracy: 0.9115 - val_loss: 0.3037\n",
            "Epoch 48/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9188 - loss: 0.2792 - val_accuracy: 0.9123 - val_loss: 0.3030\n",
            "Epoch 49/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9193 - loss: 0.2787 - val_accuracy: 0.9127 - val_loss: 0.3009\n",
            "Epoch 50/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9214 - loss: 0.2696 - val_accuracy: 0.9126 - val_loss: 0.3003\n",
            "Epoch 51/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9202 - loss: 0.2735 - val_accuracy: 0.9136 - val_loss: 0.2988\n",
            "Epoch 52/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9222 - loss: 0.2675 - val_accuracy: 0.9123 - val_loss: 0.3004\n",
            "Epoch 53/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9232 - loss: 0.2637 - val_accuracy: 0.9138 - val_loss: 0.2970\n",
            "Epoch 54/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9222 - loss: 0.2663 - val_accuracy: 0.9145 - val_loss: 0.2946\n",
            "Epoch 55/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9240 - loss: 0.2609 - val_accuracy: 0.9146 - val_loss: 0.2945\n",
            "Epoch 56/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9236 - loss: 0.2604 - val_accuracy: 0.9147 - val_loss: 0.2937\n",
            "Epoch 57/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9248 - loss: 0.2583 - val_accuracy: 0.9154 - val_loss: 0.2925\n",
            "Epoch 58/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9249 - loss: 0.2560 - val_accuracy: 0.9148 - val_loss: 0.2956\n",
            "Epoch 59/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9260 - loss: 0.2541 - val_accuracy: 0.9156 - val_loss: 0.2911\n",
            "Epoch 60/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9268 - loss: 0.2501 - val_accuracy: 0.9154 - val_loss: 0.2918\n",
            "Epoch 61/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9202 - loss: 0.2711 - val_accuracy: 0.9164 - val_loss: 0.2891\n",
            "Epoch 62/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9276 - loss: 0.2480 - val_accuracy: 0.9156 - val_loss: 0.2908\n",
            "Epoch 63/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9287 - loss: 0.2419 - val_accuracy: 0.9165 - val_loss: 0.2886\n",
            "Epoch 64/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9284 - loss: 0.2446 - val_accuracy: 0.9172 - val_loss: 0.2882\n",
            "Epoch 65/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9279 - loss: 0.2472 - val_accuracy: 0.9170 - val_loss: 0.2872\n",
            "Epoch 66/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9301 - loss: 0.2395 - val_accuracy: 0.9173 - val_loss: 0.2863\n",
            "Epoch 67/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9290 - loss: 0.2423 - val_accuracy: 0.9163 - val_loss: 0.2887\n",
            "Epoch 68/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9294 - loss: 0.2403 - val_accuracy: 0.9176 - val_loss: 0.2868\n",
            "Epoch 69/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9311 - loss: 0.2352 - val_accuracy: 0.9177 - val_loss: 0.2868\n",
            "Epoch 70/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9315 - loss: 0.2325 - val_accuracy: 0.9180 - val_loss: 0.2863\n",
            "Epoch 71/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9323 - loss: 0.2295 - val_accuracy: 0.9181 - val_loss: 0.2854\n",
            "Epoch 72/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9319 - loss: 0.2321 - val_accuracy: 0.9180 - val_loss: 0.2866\n",
            "Epoch 73/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9337 - loss: 0.2249 - val_accuracy: 0.9175 - val_loss: 0.2886\n",
            "Epoch 74/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9327 - loss: 0.2281 - val_accuracy: 0.9182 - val_loss: 0.2881\n",
            "Epoch 75/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9343 - loss: 0.2227 - val_accuracy: 0.9181 - val_loss: 0.2854\n",
            "Epoch 76/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9335 - loss: 0.2267 - val_accuracy: 0.9184 - val_loss: 0.2863\n",
            "Epoch 77/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9352 - loss: 0.2201 - val_accuracy: 0.9178 - val_loss: 0.2844\n",
            "Epoch 78/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9352 - loss: 0.2191 - val_accuracy: 0.9192 - val_loss: 0.2842\n",
            "Epoch 79/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9361 - loss: 0.2191 - val_accuracy: 0.9185 - val_loss: 0.2850\n",
            "Epoch 80/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9364 - loss: 0.2171 - val_accuracy: 0.9184 - val_loss: 0.2868\n",
            "Epoch 81/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9373 - loss: 0.2140 - val_accuracy: 0.9180 - val_loss: 0.2874\n",
            "Epoch 82/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9381 - loss: 0.2117 - val_accuracy: 0.9186 - val_loss: 0.2853\n",
            "Epoch 83/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9374 - loss: 0.2126 - val_accuracy: 0.9192 - val_loss: 0.2857\n",
            "Epoch 84/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9384 - loss: 0.2092 - val_accuracy: 0.9185 - val_loss: 0.2855\n",
            "Epoch 85/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9388 - loss: 0.2078 - val_accuracy: 0.9183 - val_loss: 0.2864\n",
            "Epoch 86/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9387 - loss: 0.2073 - val_accuracy: 0.9188 - val_loss: 0.2866\n",
            "Epoch 87/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9401 - loss: 0.2037 - val_accuracy: 0.9189 - val_loss: 0.2870\n",
            "Epoch 88/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9386 - loss: 0.2080 - val_accuracy: 0.9185 - val_loss: 0.2873\n",
            "Epoch 89/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9402 - loss: 0.2017 - val_accuracy: 0.9189 - val_loss: 0.2890\n",
            "Epoch 90/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9410 - loss: 0.2002 - val_accuracy: 0.9184 - val_loss: 0.2876\n",
            "Epoch 91/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9410 - loss: 0.1994 - val_accuracy: 0.9180 - val_loss: 0.2919\n",
            "Epoch 92/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9406 - loss: 0.2014 - val_accuracy: 0.9189 - val_loss: 0.2884\n",
            "Epoch 93/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9414 - loss: 0.1978 - val_accuracy: 0.9192 - val_loss: 0.2882\n",
            "Epoch 94/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9419 - loss: 0.1980 - val_accuracy: 0.9187 - val_loss: 0.2896\n",
            "Epoch 95/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9424 - loss: 0.1958 - val_accuracy: 0.9189 - val_loss: 0.2890\n",
            "Epoch 96/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9434 - loss: 0.1919 - val_accuracy: 0.9189 - val_loss: 0.2903\n",
            "Epoch 97/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9432 - loss: 0.1918 - val_accuracy: 0.9190 - val_loss: 0.2902\n",
            "Epoch 98/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9437 - loss: 0.1911 - val_accuracy: 0.9191 - val_loss: 0.2895\n",
            "Epoch 99/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9443 - loss: 0.1882 - val_accuracy: 0.9194 - val_loss: 0.2896\n",
            "Epoch 100/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9450 - loss: 0.1864 - val_accuracy: 0.9182 - val_loss: 0.2944\n",
            "Epoch 101/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9456 - loss: 0.1850 - val_accuracy: 0.9187 - val_loss: 0.2931\n",
            "Epoch 102/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9449 - loss: 0.1868 - val_accuracy: 0.9189 - val_loss: 0.2931\n",
            "Epoch 103/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9450 - loss: 0.1855 - val_accuracy: 0.9187 - val_loss: 0.2933\n",
            "Epoch 104/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9454 - loss: 0.1858 - val_accuracy: 0.9191 - val_loss: 0.2939\n",
            "Epoch 105/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9454 - loss: 0.1839 - val_accuracy: 0.9185 - val_loss: 0.2947\n",
            "Epoch 106/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9460 - loss: 0.1840 - val_accuracy: 0.9186 - val_loss: 0.2941\n",
            "Epoch 107/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.9469 - loss: 0.1802 - val_accuracy: 0.9185 - val_loss: 0.2946\n",
            "Epoch 108/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9463 - loss: 0.1822 - val_accuracy: 0.9188 - val_loss: 0.2943\n",
            "Epoch 109/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9468 - loss: 0.1805 - val_accuracy: 0.9186 - val_loss: 0.2946\n",
            "Epoch 110/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9464 - loss: 0.1811 - val_accuracy: 0.9194 - val_loss: 0.2940\n",
            "Epoch 111/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9469 - loss: 0.1793 - val_accuracy: 0.9182 - val_loss: 0.2975\n",
            "Epoch 112/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9475 - loss: 0.1776 - val_accuracy: 0.9191 - val_loss: 0.2965\n",
            "Epoch 113/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9482 - loss: 0.1751 - val_accuracy: 0.9184 - val_loss: 0.2976\n",
            "Epoch 114/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9480 - loss: 0.1760 - val_accuracy: 0.9190 - val_loss: 0.2974\n",
            "Epoch 115/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9480 - loss: 0.1755 - val_accuracy: 0.9187 - val_loss: 0.3011\n",
            "Epoch 116/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9481 - loss: 0.1747 - val_accuracy: 0.9187 - val_loss: 0.2988\n",
            "Epoch 117/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9494 - loss: 0.1711 - val_accuracy: 0.9183 - val_loss: 0.3009\n",
            "Epoch 118/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9492 - loss: 0.1717 - val_accuracy: 0.9179 - val_loss: 0.3018\n",
            "Epoch 119/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9492 - loss: 0.1717 - val_accuracy: 0.9188 - val_loss: 0.2996\n",
            "Epoch 120/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9491 - loss: 0.1714 - val_accuracy: 0.9189 - val_loss: 0.3004\n",
            "Epoch 121/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9497 - loss: 0.1687 - val_accuracy: 0.9187 - val_loss: 0.3023\n",
            "Epoch 122/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9499 - loss: 0.1693 - val_accuracy: 0.9183 - val_loss: 0.3028\n",
            "Epoch 123/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9501 - loss: 0.1692 - val_accuracy: 0.9189 - val_loss: 0.3016\n",
            "Epoch 124/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9504 - loss: 0.1669 - val_accuracy: 0.9189 - val_loss: 0.3031\n",
            "Epoch 125/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9508 - loss: 0.1666 - val_accuracy: 0.9186 - val_loss: 0.3045\n",
            "Epoch 126/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9512 - loss: 0.1644 - val_accuracy: 0.9182 - val_loss: 0.3042\n",
            "Epoch 127/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9509 - loss: 0.1652 - val_accuracy: 0.9187 - val_loss: 0.3061\n",
            "Epoch 128/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9510 - loss: 0.1643 - val_accuracy: 0.9189 - val_loss: 0.3051\n",
            "Epoch 129/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9518 - loss: 0.1621 - val_accuracy: 0.9187 - val_loss: 0.3050\n",
            "Epoch 130/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9513 - loss: 0.1632 - val_accuracy: 0.9186 - val_loss: 0.3067\n",
            "Epoch 131/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9512 - loss: 0.1631 - val_accuracy: 0.9189 - val_loss: 0.3061\n",
            "Epoch 132/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9524 - loss: 0.1614 - val_accuracy: 0.9184 - val_loss: 0.3072\n",
            "Epoch 133/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9520 - loss: 0.1613 - val_accuracy: 0.9189 - val_loss: 0.3081\n",
            "Epoch 134/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9523 - loss: 0.1607 - val_accuracy: 0.9190 - val_loss: 0.3078\n",
            "Epoch 135/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9530 - loss: 0.1573 - val_accuracy: 0.9190 - val_loss: 0.3082\n",
            "Epoch 136/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9536 - loss: 0.1576 - val_accuracy: 0.9185 - val_loss: 0.3094\n",
            "Epoch 137/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9536 - loss: 0.1567 - val_accuracy: 0.9187 - val_loss: 0.3091\n",
            "Epoch 138/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9541 - loss: 0.1545 - val_accuracy: 0.9188 - val_loss: 0.3093\n",
            "Epoch 139/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9540 - loss: 0.1543 - val_accuracy: 0.9183 - val_loss: 0.3110\n",
            "Epoch 140/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9541 - loss: 0.1544 - val_accuracy: 0.9183 - val_loss: 0.3126\n",
            "Epoch 141/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9539 - loss: 0.1550 - val_accuracy: 0.9194 - val_loss: 0.3107\n",
            "Epoch 142/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9539 - loss: 0.1547 - val_accuracy: 0.9186 - val_loss: 0.3133\n",
            "Epoch 143/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9543 - loss: 0.1532 - val_accuracy: 0.9188 - val_loss: 0.3127\n",
            "Epoch 144/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9544 - loss: 0.1531 - val_accuracy: 0.9187 - val_loss: 0.3132\n",
            "Epoch 145/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9549 - loss: 0.1511 - val_accuracy: 0.9188 - val_loss: 0.3134\n",
            "Epoch 146/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9550 - loss: 0.1504 - val_accuracy: 0.9185 - val_loss: 0.3148\n",
            "Epoch 147/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9544 - loss: 0.1534 - val_accuracy: 0.9191 - val_loss: 0.3138\n",
            "Epoch 148/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9551 - loss: 0.1504 - val_accuracy: 0.9187 - val_loss: 0.3154\n",
            "Epoch 149/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9555 - loss: 0.1488 - val_accuracy: 0.9181 - val_loss: 0.3199\n",
            "Epoch 150/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9551 - loss: 0.1501 - val_accuracy: 0.9183 - val_loss: 0.3171\n",
            "Epoch 151/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9552 - loss: 0.1510 - val_accuracy: 0.9182 - val_loss: 0.3169\n",
            "Epoch 152/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9555 - loss: 0.1479 - val_accuracy: 0.9189 - val_loss: 0.3175\n",
            "Epoch 153/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9559 - loss: 0.1475 - val_accuracy: 0.9188 - val_loss: 0.3162\n",
            "Epoch 154/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9557 - loss: 0.1477 - val_accuracy: 0.9185 - val_loss: 0.3179\n",
            "Epoch 155/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9554 - loss: 0.1485 - val_accuracy: 0.9185 - val_loss: 0.3186\n",
            "Epoch 156/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9566 - loss: 0.1452 - val_accuracy: 0.9186 - val_loss: 0.3205\n",
            "Epoch 157/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9567 - loss: 0.1448 - val_accuracy: 0.9187 - val_loss: 0.3210\n",
            "Epoch 158/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9562 - loss: 0.1461 - val_accuracy: 0.9190 - val_loss: 0.3203\n",
            "Epoch 159/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9567 - loss: 0.1452 - val_accuracy: 0.9182 - val_loss: 0.3224\n",
            "Epoch 160/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9572 - loss: 0.1430 - val_accuracy: 0.9190 - val_loss: 0.3224\n",
            "Epoch 161/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9569 - loss: 0.1439 - val_accuracy: 0.9189 - val_loss: 0.3214\n",
            "Epoch 162/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9569 - loss: 0.1433 - val_accuracy: 0.9186 - val_loss: 0.3233\n",
            "Epoch 163/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9573 - loss: 0.1422 - val_accuracy: 0.9187 - val_loss: 0.3217\n",
            "Epoch 164/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9572 - loss: 0.1419 - val_accuracy: 0.9188 - val_loss: 0.3250\n",
            "Epoch 165/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9572 - loss: 0.1425 - val_accuracy: 0.9182 - val_loss: 0.3239\n",
            "Epoch 166/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9578 - loss: 0.1403 - val_accuracy: 0.9185 - val_loss: 0.3258\n",
            "Epoch 167/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9582 - loss: 0.1394 - val_accuracy: 0.9179 - val_loss: 0.3275\n",
            "Epoch 168/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9580 - loss: 0.1388 - val_accuracy: 0.9191 - val_loss: 0.3255\n",
            "Epoch 169/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9577 - loss: 0.1401 - val_accuracy: 0.9182 - val_loss: 0.3268\n",
            "Epoch 170/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9580 - loss: 0.1393 - val_accuracy: 0.9187 - val_loss: 0.3273\n",
            "Epoch 171/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9585 - loss: 0.1368 - val_accuracy: 0.9185 - val_loss: 0.3274\n",
            "Epoch 172/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9582 - loss: 0.1386 - val_accuracy: 0.9177 - val_loss: 0.3292\n",
            "Epoch 173/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9587 - loss: 0.1369 - val_accuracy: 0.9187 - val_loss: 0.3285\n",
            "Epoch 174/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9586 - loss: 0.1367 - val_accuracy: 0.9181 - val_loss: 0.3320\n",
            "Epoch 175/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9591 - loss: 0.1368 - val_accuracy: 0.9179 - val_loss: 0.3312\n",
            "Epoch 176/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9592 - loss: 0.1361 - val_accuracy: 0.9185 - val_loss: 0.3305\n",
            "Epoch 177/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9592 - loss: 0.1361 - val_accuracy: 0.9184 - val_loss: 0.3295\n",
            "Epoch 178/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9590 - loss: 0.1364 - val_accuracy: 0.9179 - val_loss: 0.3326\n",
            "Epoch 179/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9593 - loss: 0.1350 - val_accuracy: 0.9186 - val_loss: 0.3308\n",
            "Epoch 180/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9594 - loss: 0.1337 - val_accuracy: 0.9180 - val_loss: 0.3318\n",
            "Epoch 181/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9592 - loss: 0.1353 - val_accuracy: 0.9185 - val_loss: 0.3329\n",
            "Epoch 182/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9591 - loss: 0.1350 - val_accuracy: 0.9181 - val_loss: 0.3346\n",
            "Epoch 183/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9595 - loss: 0.1343 - val_accuracy: 0.9186 - val_loss: 0.3317\n",
            "Epoch 184/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9595 - loss: 0.1338 - val_accuracy: 0.9185 - val_loss: 0.3356\n",
            "Epoch 185/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9599 - loss: 0.1329 - val_accuracy: 0.9184 - val_loss: 0.3343\n",
            "Epoch 186/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9596 - loss: 0.1329 - val_accuracy: 0.9187 - val_loss: 0.3365\n",
            "Epoch 187/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9598 - loss: 0.1322 - val_accuracy: 0.9181 - val_loss: 0.3347\n",
            "Epoch 188/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9601 - loss: 0.1316 - val_accuracy: 0.9186 - val_loss: 0.3353\n",
            "Epoch 189/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9603 - loss: 0.1303 - val_accuracy: 0.9179 - val_loss: 0.3383\n",
            "Epoch 190/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9598 - loss: 0.1321 - val_accuracy: 0.9181 - val_loss: 0.3390\n",
            "Epoch 191/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9609 - loss: 0.1291 - val_accuracy: 0.9178 - val_loss: 0.3389\n",
            "Epoch 192/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9608 - loss: 0.1301 - val_accuracy: 0.9185 - val_loss: 0.3381\n",
            "Epoch 193/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9604 - loss: 0.1302 - val_accuracy: 0.9187 - val_loss: 0.3385\n",
            "Epoch 194/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9604 - loss: 0.1303 - val_accuracy: 0.9185 - val_loss: 0.3402\n",
            "Epoch 195/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9610 - loss: 0.1288 - val_accuracy: 0.9185 - val_loss: 0.3400\n",
            "Epoch 196/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9609 - loss: 0.1287 - val_accuracy: 0.9183 - val_loss: 0.3395\n",
            "Epoch 197/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9612 - loss: 0.1279 - val_accuracy: 0.9179 - val_loss: 0.3416\n",
            "Epoch 198/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9610 - loss: 0.1279 - val_accuracy: 0.9181 - val_loss: 0.3451\n",
            "Epoch 199/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9608 - loss: 0.1283 - val_accuracy: 0.9174 - val_loss: 0.3442\n",
            "Epoch 200/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9612 - loss: 0.1275 - val_accuracy: 0.9184 - val_loss: 0.3429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in ingrs_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in names_token_index.items())\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
        "    preds = preds / np.sum(preds)                #\n",
        "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
        "    return np.argmax(probas)                     #\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, names_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = sample(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "i21za8cUXbmI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", ingredients_raw[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9vLyqhCYOQV",
        "outputId": "92b4354b-b272-4933-8c1a-04adad2e748d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: [\"bite size shredded rice biscuits\", \"vanilla\", \"brown sugar\", \"nuts\", \"milk\", \"butter\"]\n",
            "Decoded sentence: Salmon Pasta Salad\n",
            "\n",
            "-\n",
            "Input sentence: [\"cream of mushroom soup\", \"beef\", \"sour cream\", \"chicken breasts\"]\n",
            "Decoded sentence: Freahy Creamy Arpped Skry On Breaks\n",
            "\n",
            "-\n",
            "Input sentence: [\"frozen corn\", \"pepper\", \"cream cheese\", \"garlic powder\", \"butter\", \"salt\"]\n",
            "Decoded sentence: Pickled Pepco Fite\n",
            "\n",
            "-\n",
            "Input sentence: [\"chicken gravy\", \"cream of mushroom soup\", \"chicken\", \"shredded cheese\"]\n",
            "Decoded sentence: Ber Cheese Balls\n",
            "\n",
            "-\n",
            "Input sentence: [\"graham cracker crumbs\", \"powdered sugar\", \"peanut butter\", \"chocolate chips\", \"butter\"]\n",
            "Decoded sentence: Broccoli Corn Bread\n",
            "\n",
            "-\n",
            "Input sentence: [\"buttermilk\", \"egg\", \"sugar\", \"vanilla\", \"soda\", \"flour\", \"rhubarb\", \"butter\", \"salt\"]\n",
            "Decoded sentence: Sour Cream Pound Cake\n",
            "\n",
            "-\n",
            "Input sentence: [\"egg\", \"pepper\", \"crackers\", \"cream-style corn\", \"whole kernel corn\", \"butter\"]\n",
            "Decoded sentence: Broccoli Dip\n",
            "\n",
            "-\n",
            "Input sentence: [\"oil\", \"tomatoes\", \"green peppers\", \"water\", \"onions\", \"worcestershire sauce\"]\n",
            "Decoded sentence: Lizza Dessert\n",
            "\n",
            "-\n",
            "Input sentence: [\"condensed milk\", \"lemons\", \"graham cracker crusts\", \"pecans\", \"pineapple\"]\n",
            "Decoded sentence: Green Bean And Morzawala\n",
            "\n",
            "-\n",
            "Input sentence: [\"paraffin\", \"powdered sugar\", \"peanut butter\", \"chocolate chips\", \"butter\"]\n",
            "Decoded sentence: Egg Custard Pie\n",
            "\n",
            "-\n",
            "Input sentence: [\"flour\", \"chicken\", \"barbecue sauce\"]\n",
            "Decoded sentence: Oree Layer Pie\n",
            "\n",
            "-\n",
            "Input sentence: [\"condensed milk\", \"pie filling\", \"pineapple\", \"lemon juice\"]\n",
            "Decoded sentence: Marmalade Prun-Roblo\n",
            "\n",
            "-\n",
            "Input sentence: [\"sugar\", \"shell\", \"water\", \"cleaned strawberries\", \"cornstarch\", \"strawberry jello\", \"salt\"]\n",
            "Decoded sentence: Apple Blos'S Carile\n",
            "\n",
            "-\n",
            "Input sentence: [\"chocolate fudge cake\", \"white cake\", \"wesson oil\"]\n",
            "Decoded sentence: Lazy Cooconut Orange Barr\n",
            "\n",
            "-\n",
            "Input sentence: [\"bacon\", \"vinegar\", \"sugar\", \"green onions\", \"raisins\", \"mayonnaise\", \"broccoli\"]\n",
            "Decoded sentence: Tomato And Cheese\n",
            "\n",
            "-\n",
            "Input sentence: [\"sugar\", \"shortening\", \"cinnamon\", \"soda\", \"applesauce\", \"raisins\", \"flour\", \"nuts\"]\n",
            "Decoded sentence: Stuffed Peppers\n",
            "\n",
            "-\n",
            "Input sentence: [\"sugar\", \"shortening\", \"eggs\", \"soda\", \"bananas\", \"flour\", \"nuts\", \"salt\"]\n",
            "Decoded sentence: Springerle\n",
            "\n",
            "-\n",
            "Input sentence: [\"sour cream\", \"frango\", \"cake mix\", \"eggs\", \"chocolate fudge pudding\", \"wesson oil\", \"water\"]\n",
            "Decoded sentence: Mrea And Sea Roll\n",
            "\n",
            "-\n",
            "Input sentence: [\"garlic\", \"vegetable oil\", \"soy sauce\"]\n",
            "Decoded sentence: Cribby Rell-Sous(A\n",
            "\n",
            "-\n",
            "Input sentence: [\"egg\", \"tomato juice\", \"pepper\", \"oats\", \"onion\", \"ground beef\", \"salt\"]\n",
            "Decoded sentence: Blueberry Heaven\n",
            "\n"
          ]
        }
      ]
    }
  ]
}